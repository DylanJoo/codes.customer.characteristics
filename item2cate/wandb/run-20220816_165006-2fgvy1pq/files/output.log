  0%|          | 0/1000 [00:00<?, ?it/s]/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:990: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
  0%|          | 1/1000 [00:00<01:58,  8.40it/s]  0%|          | 3/1000 [00:00<01:33, 10.64it/s]  0%|          | 5/1000 [00:00<01:27, 11.42it/s]  1%|          | 7/1000 [00:00<01:25, 11.57it/s]  1%|          | 9/1000 [00:00<01:23, 11.81it/s]  1%|          | 11/1000 [00:00<01:23, 11.90it/s]  1%|▏         | 13/1000 [00:01<01:21, 12.09it/s]  2%|▏         | 15/1000 [00:01<01:20, 12.20it/s]  2%|▏         | 17/1000 [00:01<01:19, 12.41it/s]  2%|▏         | 19/1000 [00:01<01:19, 12.36it/s]  2%|▏         | 21/1000 [00:01<01:18, 12.45it/s]  2%|▏         | 23/1000 [00:01<01:18, 12.42it/s]  2%|▎         | 25/1000 [00:02<01:18, 12.45it/s]  3%|▎         | 27/1000 [00:02<01:18, 12.43it/s]  3%|▎         | 29/1000 [00:02<01:17, 12.55it/s]  3%|▎         | 31/1000 [00:02<01:15, 12.81it/s]  3%|▎         | 33/1000 [00:02<01:14, 13.03it/s]  4%|▎         | 35/1000 [00:02<01:13, 13.14it/s]  4%|▎         | 37/1000 [00:02<01:14, 12.91it/s]  4%|▍         | 39/1000 [00:03<01:14, 12.92it/s]  4%|▍         | 41/1000 [00:03<01:13, 13.05it/s]  4%|▍         | 43/1000 [00:03<01:12, 13.11it/s]  4%|▍         | 45/1000 [00:03<01:13, 13.02it/s]  5%|▍         | 47/1000 [00:03<01:11, 13.26it/s]  5%|▍         | 49/1000 [00:03<01:11, 13.38it/s]  5%|▌         | 51/1000 [00:04<01:10, 13.37it/s]  5%|▌         | 53/1000 [00:04<01:11, 13.20it/s]  6%|▌         | 55/1000 [00:04<01:11, 13.26it/s]  6%|▌         | 57/1000 [00:04<01:12, 12.98it/s]  6%|▌         | 59/1000 [00:04<01:12, 12.97it/s]  6%|▌         | 61/1000 [00:04<01:11, 13.14it/s]  6%|▋         | 63/1000 [00:04<01:13, 12.78it/s]  6%|▋         | 65/1000 [00:05<01:12, 12.88it/s]  7%|▋         | 67/1000 [00:05<01:12, 12.86it/s]  7%|▋         | 69/1000 [00:05<01:12, 12.93it/s]  7%|▋         | 71/1000 [00:05<01:11, 12.92it/s]  7%|▋         | 73/1000 [00:05<01:11, 13.03it/s]  8%|▊         | 75/1000 [00:05<01:10, 13.12it/s]  8%|▊         | 77/1000 [00:06<01:10, 13.18it/s]  8%|▊         | 79/1000 [00:06<01:10, 13.15it/s]  8%|▊         | 81/1000 [00:06<01:10, 13.02it/s]  8%|▊         | 83/1000 [00:06<01:12, 12.59it/s]  8%|▊         | 85/1000 [00:06<01:12, 12.57it/s]  9%|▊         | 87/1000 [00:06<01:12, 12.63it/s]  9%|▉         | 89/1000 [00:07<01:12, 12.65it/s]  9%|▉         | 91/1000 [00:07<01:11, 12.72it/s]  9%|▉         | 93/1000 [00:07<01:10, 12.81it/s] 10%|▉         | 95/1000 [00:07<01:09, 12.98it/s] 10%|▉         | 97/1000 [00:07<01:09, 12.96it/s] 10%|▉         | 99/1000 [00:07<01:10, 12.70it/s] 10%|█         | 101/1000 [00:07<01:10, 12.83it/s] 10%|█         | 103/1000 [00:08<01:09, 12.89it/s] 10%|█         | 105/1000 [00:08<01:09, 12.91it/s] 11%|█         | 107/1000 [00:08<01:08, 13.01it/s] 11%|█         | 109/1000 [00:08<01:09, 12.76it/s] 11%|█         | 111/1000 [00:08<01:10, 12.58it/s] 11%|█▏        | 113/1000 [00:08<01:10, 12.61it/s] 12%|█▏        | 115/1000 [00:09<01:10, 12.51it/s] 12%|█▏        | 117/1000 [00:09<01:10, 12.57it/s] 12%|█▏        | 119/1000 [00:09<01:10, 12.54it/s] 12%|█▏        | 121/1000 [00:09<01:08, 12.81it/s] 12%|█▏        | 123/1000 [00:09<01:09, 12.60it/s] 12%|█▎        | 125/1000 [00:09<01:08, 12.86it/s] 13%|█▎        | 127/1000 [00:09<01:07, 12.89it/s] 13%|█▎        | 129/1000 [00:10<01:06, 13.03it/s] 13%|█▎        | 131/1000 [00:10<01:06, 13.02it/s] 13%|█▎        | 133/1000 [00:10<01:06, 13.05it/s] 14%|█▎        | 135/1000 [00:10<01:06, 13.04it/s] 14%|█▎        | 137/1000 [00:10<01:06, 13.01it/s] 14%|█▍        | 139/1000 [00:10<01:05, 13.21it/s] 14%|█▍        | 141/1000 [00:11<01:04, 13.28it/s] 14%|█▍        | 143/1000 [00:11<01:04, 13.23it/s] 14%|█▍        | 145/1000 [00:11<01:04, 13.24it/s] 15%|█▍        | 147/1000 [00:11<01:04, 13.27it/s] 15%|█▍        | 149/1000 [00:11<01:04, 13.24it/s] 15%|█▌        | 151/1000 [00:11<01:04, 13.15it/s] 15%|█▌        | 153/1000 [00:11<01:04, 13.15it/s] 16%|█▌        | 155/1000 [00:12<01:04, 13.18it/s] 16%|█▌        | 157/1000 [00:12<01:04, 13.03it/s] 16%|█▌        | 159/1000 [00:12<01:05, 12.81it/s] 16%|█▌        | 161/1000 [00:12<01:06, 12.61it/s] 16%|█▋        | 163/1000 [00:12<01:05, 12.70it/s] 16%|█▋        | 165/1000 [00:12<01:04, 12.91it/s] 17%|█▋        | 167/1000 [00:13<01:03, 13.11it/s] 17%|█▋        | 169/1000 [00:13<01:02, 13.26it/s] 17%|█▋        | 171/1000 [00:13<01:02, 13.20it/s] 17%|█▋        | 173/1000 [00:13<01:03, 13.10it/s] 18%|█▊        | 175/1000 [00:13<01:03, 13.08it/s] 18%|█▊        | 177/1000 [00:13<01:02, 13.25it/s] 18%|█▊        | 179/1000 [00:13<01:03, 12.97it/s] 18%|█▊        | 181/1000 [00:14<01:03, 12.88it/s] 18%|█▊        | 183/1000 [00:14<01:02, 13.04it/s] 18%|█▊        | 185/1000 [00:14<01:03, 12.83it/s] 19%|█▊        | 187/1000 [00:14<01:03, 12.75it/s] 19%|█▉        | 189/1000 [00:14<01:03, 12.70it/s] 19%|█▉        | 191/1000 [00:14<01:03, 12.74it/s] 19%|█▉        | 193/1000 [00:15<01:03, 12.76it/s] 20%|█▉        | 195/1000 [00:15<01:02, 12.86it/s] 20%|█▉        | 197/1000 [00:15<01:03, 12.73it/s] 20%|█▉        | 199/1000 [00:15<01:03, 12.66it/s] 20%|██        | 201/1000 [00:15<01:03, 12.62it/s] 20%|██        | 203/1000 [00:15<01:03, 12.63it/s] 20%|██        | 205/1000 [00:15<01:02, 12.79it/s] 21%|██        | 207/1000 [00:16<01:01, 12.86it/s] 21%|██        | 209/1000 [00:16<01:00, 13.10it/s] 21%|██        | 211/1000 [00:16<00:59, 13.19it/s] 21%|██▏       | 213/1000 [00:16<00:59, 13.16it/s] 22%|██▏       | 215/1000 [00:16<01:00, 13.06it/s] 22%|██▏       | 217/1000 [00:16<01:00, 13.03it/s] 22%|██▏       | 219/1000 [00:17<01:01, 12.80it/s] 22%|██▏       | 221/1000 [00:17<01:00, 12.97it/s] 22%|██▏       | 223/1000 [00:17<00:59, 12.97it/s]Traceback (most recent call last):
  File "train.py", line 107, in <module>
    main()
  File "train.py", line 102, in main
    results = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1409, in train
    return inner_training_loop(
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/trainer.py", line 1651, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2345, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/trainer.py", line 2377, in compute_loss
    outputs = model(**inputs)
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/tmp2/jhju/codes.customer.characteristics/item2cate/models.py", line 44, in forward
    outputs = self.bert(
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/home/jhju/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jhju/.local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 361, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
KeyboardInterrupt
